{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/Vampire-Chan/SDUI/blob/main/ComfyUI.ipynb)\n"
      ],
      "metadata": {
        "id": "E8Oq-2xfaZSB"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bbbbbbbbbb",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "#@title **ComfyUI Environment Auto-Setup with Update Support**\n",
        "import os\n",
        "from IPython.display import clear_output\n",
        "\n",
        "# ✅ User-configurable options\n",
        "USE_GOOGLE_DRIVE = True  #@param {type:\"boolean\"}\n",
        "UPDATE_COMFY_UI = True  #@param {type:\"boolean\"}\n",
        "\n",
        "# ✅ Define workspace path\n",
        "if USE_GOOGLE_DRIVE:\n",
        "    print(\"🔗 Mounting Google Drive...\")\n",
        "    from google.colab import drive\n",
        "    drive.mount('/content/drive')\n",
        "    WORKSPACE = \"/content/drive/MyDrive/ComfyUI\"\n",
        "else:\n",
        "    WORKSPACE = \"./ComfyUI\"\n",
        "\n",
        "COMFY_REPO = \"https://github.com/comfyanonymous/ComfyUI\"\n",
        "MANAGER_REPO = \"https://github.com/ltdrdata/ComfyUI-Manager.git\"\n",
        "CUSTOM_NODES_DIR = os.path.join(WORKSPACE, \"custom_nodes\", \"ComfyUI-Manager\")\n",
        "\n",
        "# ✅ Clone or update ComfyUI\n",
        "if not os.path.exists(WORKSPACE):\n",
        "    print(\"📂 Cloning ComfyUI for the first time...\")\n",
        "    os.system(f\"git clone {COMFY_REPO} {WORKSPACE}\")\n",
        "else:\n",
        "    if UPDATE_COMFY_UI:\n",
        "        print(\"🔄 Updating ComfyUI...\")\n",
        "        os.chdir(WORKSPACE)\n",
        "        os.system(\"git reset --hard\")\n",
        "        os.system(\"git pull origin master\")\n",
        "    else:\n",
        "        print(\"⏭️ Skipping ComfyUI update.\")\n",
        "\n",
        "# ✅ Clone or update ComfyUI-Manager\n",
        "if not os.path.exists(CUSTOM_NODES_DIR):\n",
        "    print(\"📂 Cloning ComfyUI-Manager for the first time...\")\n",
        "    os.system(f\"git clone {MANAGER_REPO} {CUSTOM_NODES_DIR}\")\n",
        "else:\n",
        "    if UPDATE_COMFY_UI:\n",
        "        print(\"🔄 Updating ComfyUI-Manager...\")\n",
        "        os.chdir(CUSTOM_NODES_DIR)\n",
        "        os.system(\"git reset --hard\")\n",
        "        os.system(\"git pull origin main\")\n",
        "    else:\n",
        "        print(\"⏭️ Skipping ComfyUI-Manager update.\")\n",
        "\n",
        "# ✅ Install dependencies\n",
        "os.chdir(WORKSPACE)\n",
        "print(\"📦 Installing dependencies...\")\n",
        "os.system(\n",
        "    \"pip install xformers!=0.0.18 -r requirements.txt \"\n",
        "    \"--extra-index-url https://download.pytorch.org/whl/cu121 \"\n",
        "    \"--extra-index-url https://download.pytorch.org/whl/cu118 \"\n",
        "    \"--extra-index-url https://download.pytorch.org/whl/cu117\"\n",
        ")\n",
        "\n",
        "print(\"✅ Environment setup and update complete.\")\n",
        "clear_output()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dddddddddd",
        "cellView": "form",
        "outputId": "1965f2d1-f978-4c3f-eeb5-afec98a0768c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "🔗 Mounting Google Drive…\n",
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "⚠️ File is 30.5 GB (>15 GB). Using split workflow.\n",
            "📦 File size: 31272.29 MB\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-1-1595485322.py:43: DeprecationWarning: 'cgi' is deprecated and slated for removal in Python 3.13\n",
            "  import cgi; _, prm = cgi.parse_header(r.headers['content-disposition'])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "📥 wan2.1_i2v_480p_14B_fp16.safetensors [████--------------------------]  14.9% 4668.17/31272.29 MB |   9.2 MB/s"
          ]
        }
      ],
      "source": [
        "import os, sys, time, shutil, subprocess, urllib.parse, requests, glob\n",
        "from google.colab import drive\n",
        "\n",
        "# ───────────────────────────────────────── CONFIG\n",
        "MODEL_STORAGE_LOCATION = \"Google Drive (Persistent)\"      #@param [\"Google Drive (Persistent)\", \"Colab Filesystem (Notebook)\", \"Temporary SDUI Folder (Volatile)\"]\n",
        "MODEL_URL  = \"https://huggingface.co/Comfy-Org/Wan_2.1_ComfyUI_repackaged/resolve/main/split_files/diffusion_models/wan2.1_i2v_480p_14B_fp16.safetensors\"  # default ✔️\n",
        "API_KEY    = \"\"                                           #@param {type:\"string\"}\n",
        "MODEL_TYPE = \"checkpoints\"                                #@param [\"checkpoints\",\"clip_vision\",\"vae\",\"loras\",\"controlnet\",\"style_models\",\"upscale_models\",\"diffusion_models\",\"hypernetworks\",\"gligen\",\"custom_nodes\"]\n",
        "\n",
        "# ───────────────────────────────────────── PATHS\n",
        "def setup_environment():\n",
        "    if MODEL_STORAGE_LOCATION == \"Google Drive (Persistent)\":\n",
        "        print(\"🔗 Mounting Google Drive…\")\n",
        "        drive.mount('/content/drive')\n",
        "        return '/content/drive/MyDrive/ComfyUI'\n",
        "    elif MODEL_STORAGE_LOCATION == \"Colab Filesystem (Notebook)\":\n",
        "        return './ComfyUI'\n",
        "    else:\n",
        "        return '/content/sdui'\n",
        "\n",
        "BASE_DIR = setup_environment()\n",
        "\n",
        "MODEL_DIRS = {k: os.path.join(BASE_DIR, p) for k, p in {\n",
        "    'checkpoints':'models/checkpoints','clip_vision':'models/clip_vision','vae':'models/vae',\n",
        "    'loras':'models/loras','controlnet':'models/controlnet','style_models':'models/style_models',\n",
        "    'upscale_models':'models/upscale_models','diffusion_models':'models/diffusion_models',\n",
        "    'hypernetworks':'models/hypernetworks','gligen':'models/gligen','custom_nodes':'custom_nodes',\n",
        "    'clip':'models/clip','configs':'models/configs'}.items()}\n",
        "for p in MODEL_DIRS.values(): os.makedirs(p, exist_ok=True)\n",
        "\n",
        "# ───────────────────────────────────────── HELPERS\n",
        "def format_bar(pct, n=30):\n",
        "    done = int(pct/100*n)\n",
        "    return '[' + '█'*done + '-'*(n-done) + ']'\n",
        "\n",
        "def download_with_progress(url, dest_folder, api_key=None):\n",
        "    headers = {'Authorization': f'Bearer {api_key}'} if api_key else {}\n",
        "    r = requests.get(url, stream=True, allow_redirects=True, headers=headers)\n",
        "    r.raise_for_status()\n",
        "\n",
        "    name = os.path.basename(urllib.parse.urlparse(url).path)\n",
        "    if 'content-disposition' in r.headers:\n",
        "        import cgi; _, prm = cgi.parse_header(r.headers['content-disposition'])\n",
        "        name = prm.get('filename', name)\n",
        "\n",
        "    path   = os.path.join(dest_folder, name)\n",
        "    total  = int(r.headers.get('Content-Length', 0))\n",
        "    MB     = 1024*1024\n",
        "    print(f\"📦 File size: {total/MB:.2f} MB\")\n",
        "\n",
        "    rec = 0; t0 = time.time()\n",
        "    with open(path, 'wb') as f:\n",
        "        for chunk in r.iter_content(8192):\n",
        "            if chunk:\n",
        "                f.write(chunk); rec += len(chunk)\n",
        "                pct   = rec*100/total if total else 0\n",
        "                speed = rec/MB/max(time.time()-t0,1)\n",
        "                bar   = format_bar(pct)\n",
        "                sys.stdout.write(\n",
        "                    f\"\\r📥 {name} {bar} {pct:5.1f}% \"\n",
        "                    f\"{rec/MB:7.2f}/{total/MB:.2f} MB | {speed:5.1f} MB/s\")\n",
        "                sys.stdout.flush()\n",
        "    print(f\"\\n✅ Downloaded → {path}\")\n",
        "    return path, total\n",
        "\n",
        "# ───────────────────────────────────────── WORKFLOW\n",
        "headers = {'Authorization': f'Bearer {API_KEY}'} if API_KEY else {}\n",
        "size_bytes = int(requests.head(MODEL_URL, allow_redirects=True, headers=headers)\n",
        "                 .headers.get('Content-Length', 0))\n",
        "size_gb = size_bytes/1024/1024/1024\n",
        "\n",
        "if size_gb > 15:\n",
        "    print(f\"⚠️ File is {size_gb:.1f} GB (>15 GB). Using split workflow.\")\n",
        "    work_dir = '/content/large_model_temp'; os.makedirs(work_dir, exist_ok=True)\n",
        "    big_file, _ = download_with_progress(MODEL_URL, work_dir, API_KEY)\n",
        "\n",
        "    print(\"🔧 Splitting into 12 GB chunks…\")\n",
        "    subprocess.run(['apt','-y','install','zip'], check=False)\n",
        "    zip_base = 'WanAI_split.zip'\n",
        "    subprocess.run(['zip','-s','12000m',zip_base, os.path.basename(big_file)],\n",
        "                   cwd=work_dir, check=True)\n",
        "\n",
        "    parts = sorted(glob.glob(os.path.join(work_dir, zip_base+'*')))\n",
        "    targets = ['/content/drive/MyDrive/WanAI',\n",
        "               '/content/drive/MyDrive/WanAI2',\n",
        "               '/content/drive/MyDrive/WanAI3']\n",
        "    for t in targets: os.makedirs(t, exist_ok=True)\n",
        "\n",
        "    for idx, part in enumerate(parts):\n",
        "        tgt = targets[idx % len(targets)]\n",
        "        print(f\"📤 Copying {os.path.basename(part)} → {tgt}\")\n",
        "        shutil.copy(part, tgt)\n",
        "\n",
        "    print(\"🎉 All chunks distributed across WanAI drives.\")\n",
        "else:\n",
        "    print(f\"File is {size_gb:.1f} GB — normal download.\")\n",
        "    save_to = MODEL_DIRS.get(MODEL_TYPE, os.path.join(BASE_DIR,'models'))\n",
        "    download_with_progress(MODEL_URL, save_to, API_KEY)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jjjjjjjjjjjjjj",
        "cellView": "form",
        "outputId": "ddfda261-dfe3-48ae-b7c4-673e620584f4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 498
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "🔗 Mounting Google Drive...\n",
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "📁 Using storage at: /content/drive/MyDrive/ComfyUI\n",
            "🚀 CUDA Available!\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "numpy.dtype size changed, may indicate binary incompatibility. Expected 96 from C header, got 88 from PyObject",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-2-2ba8568a5b26>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     38\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mmod\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrequired_modules\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 40\u001b[0;31m     \u001b[0minstall_module\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmod\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     41\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m \u001b[0;31m# ✅ Download and install Cloudflared\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-2-2ba8568a5b26>\u001b[0m in \u001b[0;36minstall_module\u001b[0;34m(module)\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0minstall_module\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodule\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 33\u001b[0;31m         \u001b[0m__import__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodule\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     34\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mModuleNotFoundError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"📦 Installing missing module: {module}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/spandrel/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0m__helpers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcanonicalize\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mcanonicalize_state_dict\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0m__helpers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloader\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mModelLoader\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0m__helpers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmain_registry\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mMAIN_REGISTRY\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m from .__helpers.model_descriptor import (\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/spandrel/__helpers/loader.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mcanonicalize\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mcanonicalize_state_dict\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mmain_registry\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mMAIN_REGISTRY\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mmodel_descriptor\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mModelDescriptor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mStateDict\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mregistry\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mArchRegistry\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/spandrel/__helpers/main_registry.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0m__future__\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mannotations\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m from ..architectures import (\n\u001b[0m\u001b[1;32m      4\u001b[0m     \u001b[0mATD\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mCRAFT\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/spandrel/architectures/DAT/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0mStateDict\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m )\n\u001b[0;32m---> 13\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0m__arch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDAT\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mDAT\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/spandrel/architectures/DAT/__arch/DAT.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcheckpoint\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mcheckpoint\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0meinops\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mrearrange\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0meinops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtorch\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mRearrange\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mfunctional\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/einops/layers/torch.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mRearrangeMixin\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mReduceMixin\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0m_einmix\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0m_EinmixMixin\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_torch_specific\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mapply_for_scriptable_torch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0m__author__\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"Alex Rogozhnikov\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/einops/_torch_specific.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    126\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    127\u001b[0m \u001b[0;31m# module import automatically registers ops in torchdynamo\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 128\u001b[0;31m \u001b[0mallow_ops_in_compiled_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/einops/_torch_specific.py\u001b[0m in \u001b[0;36mallow_ops_in_compiled_graph\u001b[0;34m()\u001b[0m\n\u001b[1;32m    105\u001b[0m         \u001b[0;32mreturn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    106\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 107\u001b[0;31m         \u001b[0;32mfrom\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dynamo\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mallow_in_graph\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    108\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mImportError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    109\u001b[0m         \u001b[0mwarnings\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwarn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"allow_ops_in_compiled_graph failed to import torch: ensure pytorch >=2.0\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mImportWarning\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/_dynamo/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mconvert_frame\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0meval_frame\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresume_execution\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mbackends\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mregistry\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mlist_backends\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlookup_backend\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mregister_backend\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mcallback\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mcallback_handler\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mon_compile_end\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mon_compile_start\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/_dynamo/convert_frame.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_C\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dynamo\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mguards\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mGlobalStateGuard\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dynamo\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdistributed\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mget_compile_pg\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 33\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dynamo\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msymbolic_convert\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mTensorifyState\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     34\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_guards\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mcompile_context\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mCompileContext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mCompileId\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtracing\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_logging\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mstructured\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/_dynamo/symbolic_convert.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_logging\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 27\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dynamo\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexc\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mTensorifyScalarRestartAnalysis\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     28\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_guards\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtracing\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mTracingContext\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/_dynamo/exc.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mcounters\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/_dynamo/utils.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    109\u001b[0m             \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfft\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    110\u001b[0m             \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinalg\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 111\u001b[0;31m             \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    112\u001b[0m         )\n\u001b[1;32m    113\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/numpy/__init__.py\u001b[0m in \u001b[0;36m__getattr__\u001b[0;34m(attr)\u001b[0m\n\u001b[1;32m    335\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    336\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__dir__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 337\u001b[0;31m         \u001b[0mpublic_symbols\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mglobals\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m|\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m'testing'\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    338\u001b[0m         public_symbols -= {\n\u001b[1;32m    339\u001b[0m             \u001b[0;34m\"core\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"matrixlib\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/numpy/random/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    178\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    179\u001b[0m \u001b[0;31m# add these for module-freeze analysis (like PyInstaller)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 180\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0m_pickle\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    181\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0m_common\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    182\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0m_bounded_integers\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/numpy/random/_pickle.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mmtrand\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mRandomState\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0m_philox\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mPhilox\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0m_pcg64\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mPCG64\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mPCG64DXSM\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0m_sfc64\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mSFC64\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32mnumpy/random/mtrand.pyx\u001b[0m in \u001b[0;36minit numpy.random.mtrand\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: numpy.dtype size changed, may indicate binary incompatibility. Expected 96 from C header, got 88 from PyObject"
          ]
        }
      ],
      "source": [
        "#@title 🔧 ComfyUI Launcher (archive‑auto‑extract, pip‑cache)\n",
        "# ───────────────────────────── setup options\n",
        "storage_location = \"Google Drive\"  #@param [\"Google Drive\", \"Colab Runtime\", \"SDUI Temp Folder\"]\n",
        "archive_folders  = \"drive/MyDrive/WanAI,drive/MyDrive/WanAI2,drive/MyDrive/WanAI3\"  #@param {type:\"string\"}\n",
        "show_logs        = True  #@param {type:\"boolean\"}\n",
        "custom_flags     = \"--highvram --dont-print-server\" #@param {type:\"string\"}\n",
        "required_modules = ['torch','flask','flask_cors','requests','torchsde','spandrel','kornia','piexif','segment_anything'] #@param {type:\"string\"}\n",
        "\n",
        "import os, subprocess, threading, time, socket, glob, shutil, re\n",
        "from google.colab import drive\n",
        "\n",
        "# ───────────────────────────── mount & paths\n",
        "if storage_location == \"Google Drive\":\n",
        "    print(\"🔗 Mounting Drive…\")\n",
        "    drive.mount('/content/drive')\n",
        "    folder_path = \"/content/drive/MyDrive/ComfyUI\"\n",
        "elif storage_location == \"SDUI Temp Folder\":\n",
        "    folder_path = \"/content/sdui/ComfyUI\"\n",
        "else:\n",
        "    folder_path = \"/content/ComfyUI\"\n",
        "os.makedirs(folder_path, exist_ok=True)\n",
        "print(\"📁 Using:\", folder_path)\n",
        "\n",
        "# ───────────────────────────── pip cache (persistent wheels)\n",
        "PIP_CACHE = \"/content/drive/MyDrive/.pip_cache\"\n",
        "os.makedirs(PIP_CACHE, exist_ok=True)\n",
        "os.environ[\"PIP_CACHE_DIR\"] = PIP_CACHE\n",
        "\n",
        "def install_module(m):\n",
        "    try:\n",
        "        __import__(m)\n",
        "    except ModuleNotFoundError:\n",
        "        print(\"📦 Installing\", m)\n",
        "        subprocess.run([\"pip\",\"install\",\"-q\",m])\n",
        "\n",
        "for mod in required_modules: install_module(mod)\n",
        "\n",
        "# ───────────────────────────── archive‑extract worker\n",
        "COMFY_MODELS = os.path.join(folder_path, \"models\")\n",
        "os.makedirs(COMFY_MODELS, exist_ok=True)\n",
        "\n",
        "def extract_archives():\n",
        "    print(\"🗃️  Archive extractor thread started…\")\n",
        "    any_found=False\n",
        "    for root in [\"/content/\"+p.strip() if not p.startswith(\"/\") else \"/\"+p.strip()\n",
        "                 for p in archive_folders.split(\",\") if p.strip()]:\n",
        "        parts = glob.glob(os.path.join(root, \"*.zip.001\"))\n",
        "        for p001 in parts:\n",
        "            any_found=True\n",
        "            base = re.sub(r'\\.zip\\.001$', '', os.path.basename(p001))  # e.g. diffusion_models_wanai\n",
        "            model_type = base.split(\"_\")[0]                             # diffusion_models\n",
        "            dest_dir   = os.path.join(COMFY_MODELS, model_type)\n",
        "            os.makedirs(dest_dir,  exist_ok=True)\n",
        "\n",
        "            tmp = \"/content/tmp_zip\"; shutil.rmtree(tmp, ignore_errors=True); os.makedirs(tmp)\n",
        "            for part in glob.glob(os.path.join(root, base+\".zip*\")): shutil.copy(part, tmp)\n",
        "\n",
        "            print(f\"🔧 Extracting {base} → {model_type}\")\n",
        "            subprocess.run([\"7z\",\"x\",f\"{base}.zip\",f\"-o{dest_dir}\",\"-y\"], cwd=tmp)\n",
        "            shutil.rmtree(tmp, ignore_errors=True)\n",
        "            print(\"✅ Done:\", base)\n",
        "    if not any_found:\n",
        "        print(\"ℹ️  No split archives detected.\")\n",
        "\n",
        "threading.Thread(target=extract_archives, daemon=True).start()\n",
        "\n",
        "# ───────────────────────────── Cloudflared (same as before)\n",
        "if not os.path.exists(\"/usr/bin/cloudflared\"):\n",
        "    subprocess.run([\"wget\",\"-q\", \"https://github.com/cloudflare/cloudflared/releases/latest/download/cloudflared-linux-amd64.deb\",\"-O\",\"/tmp/cf.deb\"])\n",
        "    subprocess.run([\"dpkg\",\"-i\",\"/tmp/cf.deb\"])\n",
        "\n",
        "def tunnel(port):\n",
        "    import socket, time\n",
        "    while True:\n",
        "        try: socket.create_connection((\"127.0.0.1\", port),1); break\n",
        "        except: time.sleep(0.5)\n",
        "    p = subprocess.Popen([\"cloudflared\",\"tunnel\",\"--url\",f\"http://127.0.0.1:{port}\"],stderr=subprocess.PIPE)\n",
        "    for line in p.stderr:\n",
        "        txt=line.decode()\n",
        "        if \"trycloudflare.com\" in txt:\n",
        "            print(\"🌐 Public URL:\", txt[txt.find(\"http\"):], end='')\n",
        "\n",
        "threading.Thread(target=tunnel, daemon=True, args=(8188,)).start()\n",
        "\n",
        "# ───────────────────────────── launch ComfyUI\n",
        "print(\"🚀 Booting ComfyUI…\")\n",
        "main_py = os.path.join(folder_path, \"main.py\")\n",
        "if show_logs:\n",
        "    os.system(f\"python {main_py} {custom_flags}\")\n",
        "else:\n",
        "    subprocess.Popen([\"python\", main_py, *custom_flags.split()], stdout=subprocess.DEVNULL, stderr=subprocess.STDOUT)\n",
        "    print(\"✅ Running silently. Set 'show_logs' to True to watch output.\")\n"
      ]
    },
    {
      "source": [
        "#@title **📦 Compress & Clean Output Folder**\n",
        "#@markdown Use this tool to compress your output folder and clean it after.\n",
        "#@markdown Choose file format, compression level, and destination.\n",
        "!pip install py7zr\n",
        "!pip install rarfile\n",
        "import os\n",
        "import shutil\n",
        "import zipfile\n",
        "import sys\n",
        "import subprocess\n",
        "import py7zr\n",
        "import rarfile\n",
        "from tqdm import tqdm\n",
        "from google.colab import drive\n",
        "\n",
        "# ✅ **Mount Google Drive**\n",
        "mount_gdrive = True  #@param {type:\"boolean\"}\n",
        "if mount_gdrive:\n",
        "    drive.mount('/content/drive')\n",
        "\n",
        "# ✅ **User Options**\n",
        "output_folder = \"/content/drive/MyDrive/ComfyUI/output\"  #@param {type: \"string\"}\n",
        "archive_name = \"output\"  #@param {type: \"string\"}\n",
        "archive_destination = \"/content/drive/MyDrive/\"  #@param {type: \"string\"}\n",
        "\n",
        "file_type = \"zip\"  #@param [\"zip\", \"7z\", \"rar\"]\n",
        "compression_level = \"normal\"  #@param [\"store\", \"fast\", \"normal\", \"high\", \"maximum\"]\n",
        "\n",
        "# 🔥 **Compression Level Mapping**\n",
        "compression_map = {\n",
        "    'store': 0, 'fast': 1, 'normal': 5, 'high': 7, 'maximum': 9\n",
        "}\n",
        "\n",
        "def compress_and_clean_output(output_folder, archive_name, archive_destination, file_type, compression_level):\n",
        "    \"\"\"Compresses and cleans the output folder.\"\"\"\n",
        "\n",
        "    if not os.path.exists(output_folder):\n",
        "        print(f\"❌ Error: The folder '{output_folder}' does not exist.\")\n",
        "        return\n",
        "\n",
        "    if not os.path.exists(archive_destination):\n",
        "        print(f\"❌ Error: The destination folder '{archive_destination}' does not exist.\")\n",
        "        return\n",
        "\n",
        "    total_original_size = sum(\n",
        "        os.path.getsize(os.path.join(root, file))\n",
        "        for root, _, files in os.walk(output_folder)\n",
        "        for file in files\n",
        "    )\n",
        "\n",
        "    extension = {'zip': '.zip', '7z': '.7z', 'rar': '.rar'}[file_type]\n",
        "    archive_path = os.path.join(archive_destination, f\"{archive_name}{extension}\")\n",
        "    compression_value = compression_map[compression_level]\n",
        "\n",
        "    # ✅ **ZIP Compression**\n",
        "    if file_type == 'zip':\n",
        "        compression = zipfile.ZIP_STORED if compression_level == 'store' else zipfile.ZIP_DEFLATED\n",
        "        with zipfile.ZipFile(archive_path, 'w', compression) as zipf:\n",
        "            for root, _, files in os.walk(output_folder):\n",
        "                for file in tqdm(files, desc=\"📦 Compressing as .zip\", unit=\"file\"):\n",
        "                    zipf.write(os.path.join(root, file), os.path.relpath(os.path.join(root, file), output_folder))\n",
        "\n",
        "    # ✅ **7Z Compression**\n",
        "    elif file_type == '7z':\n",
        "        with py7zr.SevenZipFile(archive_path, 'w', filters=[{'id': py7zr.FILTER_LZMA2, 'preset': compression_value}]) as archive:\n",
        "            for root, _, files in os.walk(output_folder):\n",
        "                for file in tqdm(files, desc=\"📦 Compressing as .7z\", unit=\"file\"):\n",
        "                    archive.write(os.path.join(root, file), os.path.relpath(os.path.join(root, file), output_folder))\n",
        "\n",
        "    # ✅ **RAR Compression**\n",
        "    elif file_type == 'rar':\n",
        "        try:\n",
        "            print(\"📦 Compressing as .rar (This may take some time, progress bar not supported)\")\n",
        "            subprocess.run(['rar', 'a', '-m' + str(compression_value), archive_path, output_folder], check=True)\n",
        "        except subprocess.CalledProcessError:\n",
        "            print(\"❌ Failed to create .rar archive. Install 'rar' using 'apt-get install rar'.\")\n",
        "\n",
        "    total_compressed_size = os.path.getsize(archive_path)\n",
        "    compression_percentage = (1 - (total_compressed_size / total_original_size)) * 100 if total_original_size > 0 else 0\n",
        "\n",
        "    print(f\"📦 Original size: {total_original_size / 1024**2:.2f} MB\")\n",
        "    print(f\"📦 Compressed size: {total_compressed_size / 1024**2:.2f} MB\")\n",
        "    print(f\"📦 Compression percentage: {compression_percentage:.2f}%\")\n",
        "\n",
        "    # ✅ **Delete original files**\n",
        "    for filename in os.listdir(output_folder):\n",
        "        file_path = os.path.join(output_folder, filename)\n",
        "        try:\n",
        "            if os.path.isfile(file_path) or os.path.islink(file_path):\n",
        "                os.unlink(file_path)\n",
        "            elif os.path.isdir(file_path):\n",
        "                shutil.rmtree(file_path)\n",
        "        except Exception as e:\n",
        "            print(f'❌ Failed to delete {file_path}. Reason: {e}')\n",
        "\n",
        "    print(f\"✅ Successfully compressed and cleaned {output_folder}\")\n",
        "\n",
        "# 🔥 **Run the function**\n",
        "compress_and_clean_output(output_folder, archive_name, archive_destination, file_type, compression_level)\n"
      ],
      "cell_type": "code",
      "metadata": {
        "id": "devAdoTZi_Qa",
        "outputId": "2507d99d-d23a-4a03-cc94-f2a185d50d9d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: py7zr in /usr/local/lib/python3.11/dist-packages (0.22.0)\n",
            "Requirement already satisfied: texttable in /usr/local/lib/python3.11/dist-packages (from py7zr) (1.7.0)\n",
            "Requirement already satisfied: pycryptodomex>=3.16.0 in /usr/local/lib/python3.11/dist-packages (from py7zr) (3.22.0)\n",
            "Requirement already satisfied: pyzstd>=0.15.9 in /usr/local/lib/python3.11/dist-packages (from py7zr) (0.16.2)\n",
            "Requirement already satisfied: pyppmd<1.2.0,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from py7zr) (1.1.1)\n",
            "Requirement already satisfied: pybcj<1.1.0,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from py7zr) (1.0.3)\n",
            "Requirement already satisfied: multivolumefile>=0.2.3 in /usr/local/lib/python3.11/dist-packages (from py7zr) (0.2.3)\n",
            "Requirement already satisfied: inflate64<1.1.0,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from py7zr) (1.0.1)\n",
            "Requirement already satisfied: brotli>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from py7zr) (1.1.0)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.11/dist-packages (from py7zr) (5.9.5)\n",
            "Collecting rarfile\n",
            "  Downloading rarfile-4.2-py3-none-any.whl.metadata (4.4 kB)\n",
            "Downloading rarfile-4.2-py3-none-any.whl (29 kB)\n",
            "Installing collected packages: rarfile\n",
            "Successfully installed rarfile-4.2\n",
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "📦 Compressing as .zip: 100%|██████████| 70/70 [00:18<00:00,  3.85file/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "📦 Original size: 336.22 MB\n",
            "📦 Compressed size: 335.62 MB\n",
            "📦 Compression percentage: 0.18%\n",
            "✅ Successfully compressed and cleaned /content/drive/MyDrive/ComfyUI/output\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "yZ1B9CWu-CJr"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}